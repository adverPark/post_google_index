"""
ì‚¬ì´íŠ¸ë§µ ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹± ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ í‹°ìŠ¤í† ë¦¬ ì‚¬ì´íŠ¸ë§µ XMLì„ ë‹¤ìš´ë¡œë“œí•˜ê³  íŒŒì‹±í•˜ì—¬
ë¸”ë¡œê·¸ ê²Œì‹œê¸€ URL ëª©ë¡ì„ ì¶”ì¶œí•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import xml.etree.ElementTree as ET
from datetime import datetime
from typing import List, Dict, Optional
import requests
import re


def download_sitemap(url: str) -> Optional[str]:
    """
    ì‚¬ì´íŠ¸ë§µ XMLì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        url: ì‚¬ì´íŠ¸ë§µ URL (ì˜ˆ: https://yourblog.tistory.com/sitemap.xml)

    Returns:
        XML ë¬¸ìì—´ (ì„±ê³µ ì‹œ) ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)

    Raises:
        requests.RequestException: HTTP ìš”ì²­ ì‹¤íŒ¨ ì‹œ
    """
    try:
        # User-Agent í—¤ë”ë¥¼ ì¶”ê°€í•˜ì—¬ ë´‡ ì°¨ë‹¨ ë°©ì§€
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        # ì‚¬ì´íŠ¸ë§µ ë‹¤ìš´ë¡œë“œ (íƒ€ì„ì•„ì›ƒ 10ì´ˆ)
        response = requests.get(url, headers=headers, timeout=10)

        # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ (200ì´ ì•„ë‹ˆë©´ ì˜ˆì™¸ ë°œìƒ)
        response.raise_for_status()

        # UTF-8ë¡œ ì¸ì½”ë”©ëœ XML í…ìŠ¤íŠ¸ ë°˜í™˜
        return response.text

    except requests.RequestException as e:
        print(f"âŒ ì‚¬ì´íŠ¸ë§µ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url}")
        print(f"   ì—ëŸ¬: {str(e)}")
        return None


def parse_sitemap_urls(xml_content: str) -> List[Dict[str, str]]:
    """
    ì‚¬ì´íŠ¸ë§µ XMLì—ì„œ URL ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        xml_content: ì‚¬ì´íŠ¸ë§µ XML ë¬¸ìì—´

    Returns:
        URL ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        [{'loc': 'URL', 'lastmod': 'ìˆ˜ì •ì¼ì'}, ...]

    Note:
        - XML ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ê³ ë ¤í•˜ì—¬ íŒŒì‹±í•©ë‹ˆë‹¤
        - lastmodê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤
    """
    try:
        # XML íŒŒì‹±
        root = ET.fromstring(xml_content)

        # XML ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜ (ì‚¬ì´íŠ¸ë§µ í‘œì¤€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤)
        namespaces = {
            'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'
        }

        # URL ì •ë³´ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        urls = []

        # ëª¨ë“  <url> ìš”ì†Œë¥¼ ì°¾ì•„ì„œ ì²˜ë¦¬
        for url_element in root.findall('ns:url', namespaces):
            # <loc> íƒœê·¸ì—ì„œ URL ì¶”ì¶œ (í•„ìˆ˜)
            loc = url_element.find('ns:loc', namespaces)

            # <lastmod> íƒœê·¸ì—ì„œ ìˆ˜ì •ì¼ì ì¶”ì¶œ (ì„ íƒ)
            lastmod = url_element.find('ns:lastmod', namespaces)

            # URLì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            if loc is not None and loc.text:
                urls.append({
                    'loc': loc.text.strip(),
                    'lastmod': lastmod.text.strip() if lastmod is not None and lastmod.text else ''
                })

        return urls

    except ET.ParseError as e:
        print(f"âŒ XML íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        return []


def parse_sitemap_index(xml_content: str) -> List[str]:
    """
    ì‚¬ì´íŠ¸ë§µ ì¸ë±ìŠ¤ íŒŒì¼ì—ì„œ í•˜ìœ„ ì‚¬ì´íŠ¸ë§µ URL ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        xml_content: ì‚¬ì´íŠ¸ë§µ ì¸ë±ìŠ¤ XML ë¬¸ìì—´

    Returns:
        í•˜ìœ„ ì‚¬ì´íŠ¸ë§µ URL ë¦¬ìŠ¤íŠ¸

    Note:
        ì‚¬ì´íŠ¸ë§µ ì¸ë±ìŠ¤ëŠ” ì—¬ëŸ¬ ê°œì˜ ì‚¬ì´íŠ¸ë§µì„ í¬í•¨í•˜ëŠ” ìƒìœ„ íŒŒì¼ì…ë‹ˆë‹¤.
        ì˜ˆ: sitemap.xml -> sitemap1.xml, sitemap2.xml, ...
    """
    try:
        # XML íŒŒì‹±
        root = ET.fromstring(xml_content)

        # XML ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
        namespaces = {
            'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'
        }

        # ì‚¬ì´íŠ¸ë§µ URLì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        sitemap_urls = []

        # ëª¨ë“  <sitemap> ìš”ì†Œë¥¼ ì°¾ì•„ì„œ ì²˜ë¦¬
        for sitemap_element in root.findall('ns:sitemap', namespaces):
            # <loc> íƒœê·¸ì—ì„œ ì‚¬ì´íŠ¸ë§µ URL ì¶”ì¶œ
            loc = sitemap_element.find('ns:loc', namespaces)

            # URLì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            if loc is not None and loc.text:
                sitemap_urls.append(loc.text.strip())

        return sitemap_urls

    except ET.ParseError as e:
        print(f"âŒ ì‚¬ì´íŠ¸ë§µ ì¸ë±ìŠ¤ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        return []


def is_sitemap_index(xml_content: str) -> bool:
    """
    XMLì´ ì‚¬ì´íŠ¸ë§µ ì¸ë±ìŠ¤ì¸ì§€ ì¼ë°˜ ì‚¬ì´íŠ¸ë§µì¸ì§€ íŒë³„í•©ë‹ˆë‹¤.

    Args:
        xml_content: XML ë¬¸ìì—´

    Returns:
        True: ì‚¬ì´íŠ¸ë§µ ì¸ë±ìŠ¤
        False: ì¼ë°˜ ì‚¬ì´íŠ¸ë§µ

    Note:
        ì‚¬ì´íŠ¸ë§µ ì¸ë±ìŠ¤ëŠ” <sitemapindex> ë£¨íŠ¸ íƒœê·¸ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
        ì¼ë°˜ ì‚¬ì´íŠ¸ë§µì€ <urlset> ë£¨íŠ¸ íƒœê·¸ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
    """
    try:
        root = ET.fromstring(xml_content)
        # ë£¨íŠ¸ íƒœê·¸ ì´ë¦„ì— 'sitemapindex'ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        return 'sitemapindex' in root.tag.lower()
    except ET.ParseError:
        return False


def get_all_urls_from_sitemap(sitemap_url: str) -> List[Dict[str, str]]:
    """
    ì‚¬ì´íŠ¸ë§µì—ì„œ ëª¨ë“  URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì‚¬ì´íŠ¸ë§µ ì¸ë±ìŠ¤ì¸ ê²½ìš° ëª¨ë“  í•˜ìœ„ ì‚¬ì´íŠ¸ë§µì„ ìˆœíšŒí•©ë‹ˆë‹¤.

    Args:
        sitemap_url: ì‚¬ì´íŠ¸ë§µ URL

    Returns:
        URL ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        [{'loc': 'URL', 'lastmod': 'ìˆ˜ì •ì¼ì'}, ...]

    Note:
        - ì‚¬ì´íŠ¸ë§µ ì¸ë±ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤
        - ëª¨ë“  í•˜ìœ„ ì‚¬ì´íŠ¸ë§µì„ ìˆœíšŒí•˜ì—¬ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤
    """
    # ì‚¬ì´íŠ¸ë§µ ë‹¤ìš´ë¡œë“œ
    xml_content = download_sitemap(sitemap_url)
    if not xml_content:
        return []

    # ì‚¬ì´íŠ¸ë§µ ì¸ë±ìŠ¤ì¸ì§€ í™•ì¸
    if is_sitemap_index(xml_content):
        print(f"ğŸ“‚ ì‚¬ì´íŠ¸ë§µ ì¸ë±ìŠ¤ íŒŒì¼ ê°ì§€: {sitemap_url}")

        # í•˜ìœ„ ì‚¬ì´íŠ¸ë§µ URL ëª©ë¡ ì¶”ì¶œ
        sitemap_urls = parse_sitemap_index(xml_content)
        print(f"ğŸ“‹ í•˜ìœ„ ì‚¬ì´íŠ¸ë§µ {len(sitemap_urls)}ê°œ ë°œê²¬")

        # ëª¨ë“  URLì„ ìˆ˜ì§‘í•  ë¦¬ìŠ¤íŠ¸
        all_urls = []

        # ê° í•˜ìœ„ ì‚¬ì´íŠ¸ë§µì—ì„œ URL ì¶”ì¶œ
        for idx, sub_sitemap_url in enumerate(sitemap_urls, 1):
            print(f"ğŸ”„ [{idx}/{len(sitemap_urls)}] {sub_sitemap_url} ì²˜ë¦¬ ì¤‘...")

            # í•˜ìœ„ ì‚¬ì´íŠ¸ë§µ ë‹¤ìš´ë¡œë“œ
            sub_xml_content = download_sitemap(sub_sitemap_url)
            if sub_xml_content:
                # URL ì¶”ì¶œ ë° ë³‘í•©
                urls = parse_sitemap_urls(sub_xml_content)
                all_urls.extend(urls)
                print(f"   âœ… {len(urls)}ê°œ URL ì¶”ì¶œ")

        return all_urls

    else:
        # ì¼ë°˜ ì‚¬ì´íŠ¸ë§µì¸ ê²½ìš° ë°”ë¡œ URL ì¶”ì¶œ
        print(f"ğŸ“„ ì¼ë°˜ ì‚¬ì´íŠ¸ë§µ íŒŒì¼: {sitemap_url}")
        urls = parse_sitemap_urls(xml_content)
        print(f"âœ… {len(urls)}ê°œ URL ì¶”ì¶œ")
        return urls


def sort_urls_by_date(urls: List[Dict[str, str]], descending: bool = True) -> List[Dict[str, str]]:
    """
    URL ëª©ë¡ì„ ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.

    Args:
        urls: URL ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        descending: Trueë©´ ìµœì‹ ìˆœ(ë‚´ë¦¼ì°¨ìˆœ), Falseë©´ ì˜¤ë˜ëœìˆœ(ì˜¤ë¦„ì°¨ìˆœ)

    Returns:
        ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬ëœ URL ë¦¬ìŠ¤íŠ¸

    Note:
        - lastmod ê°’ì´ ì—†ëŠ” URLì€ ì •ë ¬ ìš°ì„ ìˆœìœ„ê°€ ë‚®ìŠµë‹ˆë‹¤
        - ISO 8601 í˜•ì‹ì˜ ë‚ ì§œë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤ (ì˜ˆ: 2025-01-15T10:30:25+09:00)
    """
    def get_date_key(url_info: Dict[str, str]) -> datetime:
        """ì •ë ¬ í‚¤ë¥¼ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        lastmod = url_info.get('lastmod', '')

        if not lastmod:
            # lastmodê°€ ì—†ëŠ” ê²½ìš° ê°€ì¥ ì˜¤ë˜ëœ ë‚ ì§œë¡œ ì„¤ì •
            return datetime.min

        try:
            # ISO 8601 í˜•ì‹ ë‚ ì§œ íŒŒì‹±
            # ì˜ˆ: 2025-01-15T10:30:25+09:00
            # íƒ€ì„ì¡´ ì •ë³´ë¥¼ ì œê±°í•˜ê³  íŒŒì‹± (ê°„ë‹¨í•œ ì²˜ë¦¬)
            date_str = lastmod.split('+')[0].split('Z')[0]
            return datetime.fromisoformat(date_str)
        except (ValueError, AttributeError):
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê°€ì¥ ì˜¤ë˜ëœ ë‚ ì§œë¡œ ì„¤ì •
            return datetime.min

    # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬
    return sorted(urls, key=get_date_key, reverse=descending)


def filter_post_urls(urls: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """
    URL ëª©ë¡ì—ì„œ ë¸”ë¡œê·¸ ê¸€ í˜ì´ì§€ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    ì¹´í…Œê³ ë¦¬, íƒœê·¸, ë©”ì¸ í˜ì´ì§€, ëª¨ë°”ì¼ URL ë“±ì€ ì œì™¸ë©ë‹ˆë‹¤.

    Args:
        urls: URL ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸

    Returns:
        ë¸”ë¡œê·¸ ê¸€ í˜ì´ì§€ë§Œ í¬í•¨ëœ URL ë¦¬ìŠ¤íŠ¸

    Note:
        í‹°ìŠ¤í† ë¦¬ URL íŒ¨í„´:
        - ê¸€ í˜ì´ì§€: https://blog.tistory.com/123 ë˜ëŠ” https://blog.tistory.com/notice/123
        - ì œì™¸ ëŒ€ìƒ: /category/, /tag/, /pages/, /m/ (ëª¨ë°”ì¼), ë©”ì¸ í˜ì´ì§€ ë“±
    """
    post_urls = []

    for url_info in urls:
        url = url_info['loc']

        # ëª¨ë°”ì¼ URL ì œì™¸ (/m/ë¡œ ì‹œì‘í•˜ëŠ” ê²½ë¡œ)
        # ì˜ˆ: https://blog.tistory.com/m/123 (ì œì™¸)
        if '/m/' in url:
            continue

        # ê¸€ í˜ì´ì§€ íŒ¨í„´: ë„ë©”ì¸/ìˆ«ì ë˜ëŠ” ë„ë©”ì¸/ê²½ë¡œ/ìˆ«ì
        # ì˜ˆ: https://blog.tistory.com/123
        #     https://blog.tistory.com/notice/456
        if re.match(r'^https?://[^/]+/(\w+/)?(\d+)$', url):
            # ì¹´í…Œê³ ë¦¬, íƒœê·¸, í˜ì´ì§€ ë“±ì€ ì œì™¸
            if not any(excluded in url for excluded in ['/category', '/tag', '/page']):
                post_urls.append(url_info)

    return post_urls


def sort_urls_by_number(urls: List[Dict[str, str]], descending: bool = True) -> List[Dict[str, str]]:
    """
    URL ëª©ë¡ì„ URL ë²ˆí˜¸ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.

    Args:
        urls: URL ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        descending: Trueë©´ í° ë²ˆí˜¸ë¶€í„°(ë‚´ë¦¼ì°¨ìˆœ), Falseë©´ ì‘ì€ ë²ˆí˜¸ë¶€í„°(ì˜¤ë¦„ì°¨ìˆœ)

    Returns:
        URL ë²ˆí˜¸ìˆœìœ¼ë¡œ ì •ë ¬ëœ URL ë¦¬ìŠ¤íŠ¸

    Note:
        - URLì—ì„œ ë§ˆì§€ë§‰ ìˆ«ìë¥¼ ì¶”ì¶œí•˜ì—¬ ì •ë ¬í•©ë‹ˆë‹¤
        - ì˜ˆ: /407 > /406 > /405 (descending=True)
        - ìˆ«ìë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ëŠ” URLì€ ìš°ì„ ìˆœìœ„ê°€ ë‚®ìŠµë‹ˆë‹¤
    """
    def get_url_number(url_info: Dict[str, str]) -> int:
        """URLì—ì„œ ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        url = url_info.get('loc', '')

        # URL ë§ˆì§€ë§‰ ë¶€ë¶„ì—ì„œ ìˆ«ì ì¶”ì¶œ
        # ì˜ˆ: https://blog.tistory.com/123 -> 123
        #     https://blog.tistory.com/notice/456 -> 456
        match = re.search(r'/(\d+)$', url)
        if match:
            return int(match.group(1))

        # ìˆ«ìë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° 0 ë°˜í™˜ (ìš°ì„ ìˆœìœ„ ë‚®ìŒ)
        return 0

    # URL ë²ˆí˜¸ìˆœìœ¼ë¡œ ì •ë ¬
    return sorted(urls, key=get_url_number, reverse=descending)
